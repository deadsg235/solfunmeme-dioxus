use walkdir::WalkDir;
use std::fs;
use std::collections::HashMap;
use solfunmeme_core_logic::code_analyzer::CodeAnalyzer;
use std::path::Path;
use std::time::{SystemTime, UNIX_EPOCH};
use std::collections::BTreeMap;
use regex::Regex;
use std::io::Write;

/**
idea : lets build a mini compiler right here
first filter out only rust files.
refactor this code into parts.
read the source
parse to ast
embed source into vector
report and identify duplicates

construction of functions to match struture of the code and 
reporting on the matches
analysis of the of the statistics of the matches
construction of new functions based on the statistics
construction of new functions based on the matches
construction of new functions based on the analysis
construction of new functions based on the reporting
construction of new functions based on the extraction
construction of new functions based on the mathematical model
construction of new functions based on the export
a recursive feedback loop using LLMS and statistics to dynamically create new functions and models in real time.


extraction of terms from identifiers
extraction of relcationships between terms
creation of mathematical mmodel

export data for gui (reduce the size)

 */

// --- AST Node Type Emoji Mapping (updated & deduplicated) ---
const EMOJI_TYPE_MAP: &[(&str, &str, &str)] = &[
    // Rust Core
    ("fn", "ü¶Ä‚öôÔ∏è", "Rust Core"),
    ("struct", "üèõÔ∏èüß±", "Rust Core"),
    ("enum", "üé≤", "Rust Core"),
    ("mod", "üì¶", "Rust Core"),
    ("use", "üîó", "Rust Core"),
    ("impl", "üî®", "Rust Core"),
    ("trait", "üß©", "Rust Core"),
    ("const", "üîí", "Rust Core"),
    ("static", "ü™®", "Rust Core"),
    ("type", "üè∑Ô∏è", "Rust Core"),
    ("ident", "üÜî", "Rust Core"),
    ("attrs", "üé®", "Rust Core"),
    ("fields", "üå±", "Rust Core"),
    ("meta", "üß†", "Rust Core"),
    ("path", "üõ§Ô∏è", "Rust Core"),
    ("lit", "üí°", "Rust Core"),
    ("tokens", "üéüÔ∏è", "Rust Core"),
    ("expr", "üßÆ", "Rust Core"),
    ("block", "üß±", "Rust Core"),
    ("call", "üìû", "Rust Core"),
    ("method", "üîß", "Rust Core"),
    ("macro", "ü™Ñ", "Rust Core"),
    ("trait_object", "ü¶ã", "Rust Core"),
    ("item", "üìú", "Rust Core"),
    ("items", "üìö", "Rust Core"),
    ("field", "üåø", "Rust Core"),
    ("inputs", "‚û°Ô∏è", "Rust Core"),
    ("output", "‚¨ÖÔ∏è", "Rust Core"),
    ("receiver", "üì°", "Rust Core"),
    ("generics", "üî£", "Rust Core"),
    ("lifetime", "‚è≥", "Rust Core"),
    ("where_clause", "‚ùì", "Rust Core"),
    ("tuple", "ü§ù", "Rust Core"),
    ("tuple_struct", "üèóÔ∏è", "Rust Core"),
    ("array", "üî¢", "Rust Core"),
    ("int", "#Ô∏è‚É£", "Rust Core"),
    ("float", "üíß", "Rust Core"),
    ("bool", "‚úÖ", "Rust Core"),
    ("char", "üî§", "Rust Core"),
    ("str", "üìù", "Rust Core"),
    ("closure", "üï∏Ô∏è", "Rust Core"),
    ("let", "üìå", "Rust Core"),
    ("match", "üéØ", "Rust Core"),
    ("if", "‚ùì", "Rust Core"),
    ("else_branch", "üîÑ", "Rust Core"),
    ("then_branch", "‚û°Ô∏è", "Rust Core"),
    ("for_loop", "üîÅ", "Rust Core"),
    ("while", "üîÇ", "Rust Core"),
    ("loop", "‚ôæÔ∏è", "Rust Core"),
    ("return", "‚Ü©Ô∏è", "Rust Core"),
    ("break", "‚õî", "Rust Core"),
    ("continue", "‚ñ∂Ô∏è", "Rust Core"),
    ("assign", "üìù", "Rust Core"),
    ("op", "‚öôÔ∏è", "Rust Core"),
    ("unary", "‚ûñ", "Rust Core"),
    ("binary", "‚ûó", "Rust Core"),
    ("cast", "üîÄ", "Rust Core"),
    ("index", "üìç", "Rust Core"),
    ("range", "‚ÜîÔ∏è", "Rust Core"),
    ("slice", "üç∞", "Rust Core"),
    ("macro_rules", "üìê", "Rust Core"),
    ("group", "üë•", "Rust Core"),
    ("delim", "üöß", "Rust Core"),
    ("punct", "‚ÄºÔ∏è", "Rust Core"),
    ("paren", "( )", "Rust Core"),
    ("bracket", "[ ]", "Rust Core"),
    ("brace", "{ }", "Rust Core"),
    ("attr", "üñºÔ∏è", "Rust Core"),
    ("name_value", "üîë", "Rust Core"),
    ("value", "üíé", "Rust Core"),
    ("style", "üé®", "Rust Core"),
    ("method_call", "üì≤", "Rust Core"),
    ("dyn", "üåÄ", "Rust Core"),
    ("mut", "üîÑ", "Rust Core"),
    ("ref", "üîó", "Rust Core"),
    ("self_ty", "üÜî", "Rust Core"),
    ("super", "üåü", "Rust Core"),
    ("crate", "üöö", "Rust Core"),
    ("macro_input", "üì•", "Rust Core"),
    ("macro_output", "üì¶", "Rust Core"),
    ("params", "‚öôÔ∏è", "Rust Core"),
    ("args", "üì¢", "Rust Core"),
    ("arguments", "üéôÔ∏è", "Rust Core"),
    ("arm", "üõ°Ô∏è", "Rust Core"),
    ("arms", "üõ†Ô∏è", "Rust Core"),
    ("variant", "üé≠", "Rust Core"),
    ("variants", "üî£", "Rust Core"),
    ("fields_named", "üè∑Ô∏è", "Rust Core"),
    ("fields_unnamed", "üåø", "Rust Core"),
    ("pat", "üñºÔ∏è", "Rust Core"),
    ("stmt", "üñãÔ∏è", "Rust Core"),
    ("stmts", "üìú", "Rust Core"),
    ("ty", "üîñ", "Rust Core"),
    ("bound", "‚õìÔ∏è", "Rust Core"),
    ("bounds", "üîó", "Rust Core"),
    ("vis", "üëÄ", "Rust Core"),
    ("list", "‚úÖ", "Rust Core"),
    ("token", "üéüÔ∏è", "Rust Core"),
    ("tree", "üå≥", "Rust Core"),
    ("segment", "üß©", "Rust Core"),
    ("segments", "üß©", "Rust Core"),
    ("assoc_type", "üîó", "Rust Core"),
    ("async", "‚è©", "Rust Core"),
    ("await", "‚è≥", "Rust Core"),
    ("base", "üèÅ", "Rust Core"),
    ("body", "üèÉ", "Rust Core"),
    ("colon_token", ":", "Rust Core"),
    ("delimiter", "üöß", "Rust Core"),
    ("angle_bracketed", "‚ü®‚ü©", "Rust Core"),
    ("cond", "‚ùì", "Rust Core"),
    ("func", "ü¶Ä", "Rust Core"),
    ("init", "üö¶", "Rust Core"),
    ("right", "üëâ", "Rust Core"),
    ("semi", ";", "Rust Core"),
    ("semi_token", ";", "Rust Core"),
    ("spacing", "‚ÜîÔ∏è", "Rust Core"),
    ("start", "üîú", "Rust Core"),
    ("stream", "üåä", "Rust Core"),
    ("try", "ü§û", "Rust Core"),
    ("bare_fn", "ü¶Ä", "Rust Core"),
    ("bounded_ty", "üìè", "Rust Core"),
    ("byte_str", "üíæ", "Rust Core"),
    ("cases", "üé≠", "Rust Core"),
    ("dot2_token", "‚Ä¢", "Rust Core"),
    ("elem", "üì¶", "Rust Core"),
    ("elems", "üì¶", "Rust Core"),
    ("end", "üîö", "Rust Core"),
    ("impl_trait", "üß©", "Rust Core"),
    ("left", "üëà", "Rust Core"),
    ("len", "üìè", "Rust Core"),
    ("limits", "üìè", "Rust Core"),
    ("move", "üöö", "Rust Core"),
    ("named", "üè∑Ô∏è", "Rust Core"),
    ("or", "üîÄ", "Rust Core"),
    ("parenthesized", "( )", "Rust Core"),
    ("reference", "üîó", "Rust Core"),
    ("rename", "üìù", "Rust Core"),
    ("repeat", "üîÅ", "Rust Core"),
    ("rest", "üîÅ", "Rust Core"),
    ("restricted", "üö´", "Rust Core"),
    ("turbofish", "üêü", "Rust Core"),
    ("typed", "üè∑Ô∏è", "Rust Core"),
    ("unnamed", "üè∑Ô∏è", "Rust Core"),
    ("unsafe", "‚ò¢Ô∏è", "Rust Core"),
    // Web/CSS
    ("px", "üìè", "Web/CSS"), ("deg", "üß≠", "Web/CSS"), ("em", "üî†", "Web/CSS"), ("rem", "üî°", "Web/CSS"), ("vh", "üìê", "Web/CSS"), ("vw", "üìè", "Web/CSS"), ("s", "‚è±Ô∏è", "Web/CSS"), ("ms", "‚è≤Ô∏è", "Web/CSS"),
    ("animation", "üéûÔ∏è", "Web/CSS"), ("transition", "üîÑ", "Web/CSS"), ("absolute", "üìê", "Web/CSS"), ("align", "üìè", "Web/CSS"), ("app", "üì±", "Web/CSS"), ("app_state", "üóÑÔ∏è", "Web/CSS"), ("accessibility", "‚ôø", "Web/CSS"),
    ("adapter", "üîå", "Web/CSS"), ("actions", "üé¨", "Web/CSS"), ("action", "üé¨", "Web/CSS"), ("active", "üî•", "Web/CSS"),
    // Crypto/Security/Systems
    ("aead", "üîí", "Crypto"), ("aeads", "üîí", "Crypto"), ("aes", "üîë", "Crypto"), ("argon2", "üßÇ", "Crypto"), ("arc", "üß≤", "Crypto"), ("addr2line", "üìç", "Crypto"), ("aarch64", "üì¶", "Crypto"), ("amd64", "üíª", "Crypto"), ("armv8", "üí™", "Crypto"),
    ("crypto", "üîí", "Crypto"), ("curve25519", "‚û∞", "Crypto"), ("ed25519", "üìù", "Crypto"), ("elliptic", "‚û∞", "Crypto"), ("fiat", "üíµ", "Crypto"), ("cbor", "üì¶", "Crypto"),
    // Project-specific
    ("agave", "üåµ", "Project-Specific"), ("helius", "üåû", "Project-Specific"),
    // Internationalization
    ("icu4x", "üåê", "Internationalization"), ("cldr", "üåç", "Internationalization"), ("chinese", "üÄÑ", "Internationalization"), ("hebrew", "‚ú°Ô∏è", "Internationalization"), ("coptic", "‚õ™", "Internationalization"), ("ethiopic", "üåÑ", "Internationalization"), ("calendar", "üìÖ", "Internationalization"), ("datetime", "‚è∞", "Internationalization"),
    // Testing/Benchmarking
    ("criterion", "‚è±Ô∏è", "Testing"), ("benches", "üèãÔ∏è", "Testing"), ("fuzz", "üß™", "Testing"), ("examples", "üìö", "Testing"), ("docs", "üìñ", "Testing"),
    // Misc/General
    ("algebra", "‚ûó", "General"), ("analysis", "üîç", "General"), ("analyze", "üî¨", "General"), ("account", "üë§", "General"), ("accounts", "üë•", "General"),
    // Suffixes for versioning/hashes
    ("zm", "üß¨", "Versioning"), ("h", "‚è≥", "Versioning"), ("v", "üî¢", "Versioning"),
    // Color codes (hex)
    ("ff", "üé®", "Color"), ("00", "‚ö´", "Color"), ("ffffff", "‚¨ú", "Color"), ("000000", "‚¨õ", "Color"),
    // Numbers (for fun)
    ("0", "0Ô∏è‚É£", "Numbers"), ("1", "1Ô∏è‚É£", "Numbers"), ("2", "2Ô∏è‚É£", "Numbers"), ("3", "3Ô∏è‚É£", "Numbers"), ("4", "4Ô∏è‚É£", "Numbers"), ("5", "5Ô∏è‚É£", "Numbers"), ("6", "6Ô∏è‚É£", "Numbers"), ("7", "7Ô∏è‚É£", "Numbers"), ("8", "8Ô∏è‚É£", "Numbers"), ("9", "9Ô∏è‚É£", "Numbers"), ("10", "üîü", "Numbers"), ("100", "üíØ", "Numbers"), ("255", "üüß", "Numbers"),
    // Emoji codepoints
    ("1f3a8", "üé®", "Emoji"), ("1f4dd", "üìù", "Emoji"), ("1f680", "üöÄ", "Emoji"), ("1f4a9", "üí©", "Emoji"),
    // Heuristic/structural
    ("byte", "üíæ", "Numbers"), ("parenthes", "( )", "Rust Core"), ("case", "üé≠", "Rust Core"), ("dot", "‚Ä¢", "General"), ("colon", ":", "General"), ("bounded", "üìè", "General"),
    ("_", "‚¨ú", "Rust Core"), ("colon2_token", ":", "Rust Core"), ("cond", "‚ùì", "Rust Core"), ("content", "üì¶", "General"), ("if", "‚ùì", "Rust Core"), ("where_clause", "üìú", "Rust Core"),
    // Dependency Management
    ("cargo", "üì¶", "Dependencies"), ("toml", "üìã", "Dependencies"), ("lock", "üîí", "Dependencies"), ("deps", "üîó", "Dependencies"), ("dependencies", "üîó", "Dependencies"), ("dev_deps", "üß™", "Dependencies"), ("features", "‚ú®", "Dependencies"), ("workspace", "üè¢", "Dependencies"), ("member", "üë§", "Dependencies"), ("path", "üõ§Ô∏è", "Dependencies"), ("git", "üêô", "Dependencies"), ("branch", "üåø", "Dependencies"), ("tag", "üè∑Ô∏è", "Dependencies"), ("version", "üî¢", "Dependencies"), ("semver", "üìä", "Dependencies"), ("patch", "ü©π", "Dependencies"), ("minor", "üìà", "Dependencies"), ("major", "üöÄ", "Dependencies"), ("optional", "‚ùì", "Dependencies"), ("default_features", "‚öôÔ∏è", "Dependencies"), ("no_default_features", "üö´", "Dependencies"),
    // Build System
    ("build", "üî®", "Build"), ("target", "üéØ", "Build"), ("release", "üöÄ", "Build"), ("debug", "üêõ", "Build"), ("profile", "üìä", "Build"), ("optimization", "‚ö°", "Build"), ("compiler", "‚öôÔ∏è", "Build"), ("linker", "üîó", "Build"), ("artifact", "üé®", "Build"), ("binary", "üíª", "Build"), ("library", "üìö", "Build"), ("static", "ü™®", "Build"), ("dynamic", "üåä", "Build"), ("wasm", "üåê", "Build"), ("native", "üè†", "Build"), ("cross_compile", "üîÑ", "Build"), ("platform", "üñ•Ô∏è", "Build"), ("architecture", "üèóÔ∏è", "Build"), ("triple", "üéØ", "Build"), ("toolchain", "üîß", "Build"),
    // Package Management
    ("crate", "üì¶", "Packages"), ("package", "üì¶", "Packages"), ("registry", "üìö", "Packages"), ("publish", "üì§", "Packages"), ("yank", "üóëÔ∏è", "Packages"), ("download", "üì•", "Packages"), ("install", "‚öôÔ∏è", "Packages"), ("uninstall", "üóëÔ∏è", "Packages"), ("update", "üîÑ", "Packages"), ("upgrade", "‚¨ÜÔ∏è", "Packages"), ("downgrade", "‚¨áÔ∏è", "Packages"), ("pin", "üìå", "Packages"), ("unpin", "üìå", "Packages"), ("vendor", "üè™", "Packages"), ("offline", "üì¥", "Packages"), ("mirror", "ü™û", "Packages"), ("source", "üìÑ", "Packages"), ("repository", "üìö", "Packages"), ("fork", "üç¥", "Packages"), ("clone", "üìã", "Packages"),
    // Module System
    ("mod", "üì¶", "Modules"), ("module", "üì¶", "Modules"), ("pub", "üëÄ", "Modules"), ("private", "üîí", "Modules"), ("re_export", "üì§", "Modules"), ("use", "üîó", "Modules"), ("extern", "üåç", "Modules"), ("crate", "üì¶", "Modules"), ("super", "‚¨ÜÔ∏è", "Modules"), ("self", "üÜî", "Modules"), ("as", "üè∑Ô∏è", "Modules"), ("glob", "üåü", "Modules"), ("prelude", "üé≠", "Modules"), ("lib", "üìö", "Modules"), ("main", "üéØ", "Modules"), ("bin", "üíª", "Modules"), ("example", "üìù", "Modules"), ("test", "üß™", "Modules"), ("bench", "üèãÔ∏è", "Modules"), ("doc", "üìñ", "Modules"),
    // Project Structure
    ("src", "üìÅ", "Structure"), ("tests", "üß™", "Structure"), ("examples", "üìù", "Structure"), ("benches", "üèãÔ∏è", "Structure"), ("docs", "üìñ", "Structure"), ("assets", "üé®", "Structure"), ("resources", "üì¶", "Structure"), ("config", "‚öôÔ∏è", "Structure"), ("scripts", "üìú", "Structure"), ("tools", "üîß", "Structure"), ("build_scripts", "üî®", "Structure"), ("proc_macro", "ü™Ñ", "Structure"), ("macro_rules", "üìê", "Structure"), ("derive", "üß¨", "Structure"), ("attribute", "üè∑Ô∏è", "Structure"), ("annotation", "üìù", "Structure"), ("metadata", "üìä", "Structure"), ("manifest", "üìã", "Structure"), ("license", "üìú", "Structure"), ("readme", "üìñ", "Structure"), ("changelog", "üìù", "Structure"),
    ("binary", "‚ö´‚ö™", "Operators"),
    ("unary", "‚ûñ", "Operators"),
    ("embed", "üì¶‚ú®", "Assets"),
    ("our", "üë•üíñ", "Pronouns"),
    ("folder", "üìÅüå∏", "Filesystem"),
    ("rust_embed", "ü¶Äüì¶üåü", "Assets"),
    ("extractor", "üß≤üíé", "Processing"),
    ("borsh", "üì¶üß©", "Serialization"),
    ("windows", "ü™üüåà", "Platform"),
    ("crates", "üì¶üß∞", "Rust"),
    ("rs", "ü¶Äüåü", "Rust"),
    ("data", "üíæüìä", "Data"),
    ("provider", "ü§ùüåü", "Service"),
    ("size", "üìèüìê", "Measurement"),
    ("idx", "#Ô∏è‚É£üî¢", "Indexing"),
    ("libs", "üìö‚ú®", "Rust"),
    ("rust", "ü¶Äüí´", "Rust"),
    ("de", "‚¨áÔ∏è", "Serialization"),
    ("ser", "‚¨ÜÔ∏è", "Serialization"),
    ("github", "üêô", "Platform"),
    ("bincode", "üî¢", "Serialization"),
    ("report", "üìÑ", "Reporting"),
    ("speedy", "‚ö°", "Performance"),
    ("curves", "‚û∞", "Math"),
    ("change", "üîÑ", "General"),
    ("string", "üî§", "Data"),
    ("leptos", "üåø", "Framework"),
    ("class", "üè∑Ô∏è", "OOP"),
    ("full", "üàµ", "General"),
    ("objc2", "üçè", "Platform"),
    ("text", "üìù", "Data"),
    ("dioxus", "üß¨", "Framework"),
    ("emoji", "üòÄ‚ú®", "Meta"),
    ("vector", "‚û°Ô∏èüü¶", "Math"),
    ("mapping", "üó∫Ô∏èüîó", "Meta"),
    ("profile", "üë§üìã", "Meta"),
    ("pattern", "üî∂üîç", "Meta"),
    ("scanner", "üîéüì°", "Tool"),
    ("filter", "üöøüî¨", "Tool"),
    ("sort", "üî¢‚¨ÜÔ∏è", "Tool"),
    ("frequency", "üìä", "Stats"),
    ("coverage", "üó∫Ô∏èüåà", "Stats"),
    ("identifier", "üÜîüî§", "Syntax"),
    ("literal", "üî§üíé", "Syntax"),
    ("module", "üì¶üß©", "Code"),
    ("pipeline", "‚õìÔ∏èüöÄ", "System"),
    ("graph", "üìàüîó", "System"),
    ("incremental", "‚ûï‚è≥", "System"),
    ("compile", "‚öôÔ∏èüö¶", "System"),
    ("analysis", "üî¨üß†", "System"),
    ("GUI", "üñ•Ô∏èüé®", "UI"),
    ("MCP", "üß©üõ†Ô∏è", "System"),
    ("interactive", "üñ±Ô∏èüí¨", "UI"),
    ("review", "üëÄüìù", "Process"),
    ("assign", "‚û°Ô∏èüè∑Ô∏è", "Process"),
    ("vocabulary", "üìöüó£Ô∏è", "Meta"),
    ("automation", "ü§ñ‚ö°", "System"),
    ("beauty", "üå∏‚ú®", "Meta"),
    ("layer", "üßÖüé®", "Meta"),
    ("rich", "üíéüåà", "Meta"),
    ("expressive", "üé≠üí¨", "Meta"),
    ("system", "üñß‚öôÔ∏è", "System"),
    ("codebase", "üíªüèóÔ∏è", "Code"),
    ("component", "üß©üèóÔ∏è", "Code"),
    ("crate", "üì¶ü¶Ä", "Rust"),
    ("signal", "üì∂üîî", "State"),
    ("state", "üîîüß†", "State"),
    ("dynamic", "üîÑ‚ö°", "Meta"),
    ("static", "üõëüèõÔ∏è", "Meta"),
    ("modular", "üß©üîó", "Meta"),
    ("scan", "üîçüì°", "Tool"),
    ("extract", "üß≤üì§", "Tool"),
    ("analyze", "üî¨üß†", "Tool"),
    ("visual", "üëÅÔ∏èüé®", "UI"),
    ("semantic", "üß†üî§", "Meta"),
    ("syntax", "üî§üìê", "Meta"),
    ("ast", "üå≥üß©", "Code"),
    ("node", "üîóüå≥", "Code"),
    ("token", "üîñüî§", "Code"),
    ("snippet", "‚úÇÔ∏èüìÑ", "Code"),
    ("example", "üí°üìÑ", "Meta"),
    ("template", "üìÑüß©", "Meta"),
    ("category", "üè∑Ô∏èüìö", "Meta"),
    ("term", "üî§üè∑Ô∏è", "Meta"),
    ("unmapped", "‚ùìüö´", "Stats"),
    ("mapped", "‚úÖüîó", "Stats"),
    ("new", "‚ú®", "Creation"),
    ("transaction", "üîóüí∞", "Blockchain"),
    ("solana", "‚ö°", "Blockchain"),
    ("workflows", "üó∫Ô∏è", "Process"),
    ("packages", "üì¶", "Packaging"),
    ("property", "üîë", "Attribute"),
    ("component", "üß©", "Architecture"),
    ("framework", "üèóÔ∏è", "Architecture"),
    ("hashes", "#Ô∏è‚É£", "Crypto"),
    ("programs", "‚öôÔ∏è", "Execution"),
    ("11", "üî¢üåü", "Math"),
    // --- Meta/Workflow Concepts ---
    ("refactor", "üõ†Ô∏èüîÑ", "Workflow"),
    ("modular", "üß©üîó", "Workflow"),
    ("vectorize", "‚û°Ô∏èüü¶", "Workflow"),
    ("visualize", "üëÅÔ∏èüé®", "Workflow"),
    ("automate", "ü§ñ‚ö°", "Workflow"),
    ("dynamic", "üîÑ‚ö°", "Workflow"),
    ("static", "üõëüèõÔ∏è", "Workflow"),
    ("interactive", "üñ±Ô∏èüí¨", "Workflow"),
    ("pipeline", "‚õìÔ∏èüöÄ", "Workflow"),
    ("graph", "üìàüîó", "Workflow"),
    ("scan", "üîçüì°", "Workflow"),
    ("extract", "üß≤üì§", "Workflow"),
    ("analyze", "üî¨üß†", "Workflow"),
    ("filter", "üöøüî¨", "Workflow"),
    ("sort", "üî¢‚¨ÜÔ∏è", "Workflow"),
    ("review", "üëÄüìù", "Workflow"),
    ("assign", "‚û°Ô∏èüè∑Ô∏è", "Workflow"),
    ("report", "üìÑüìä", "Workflow"),
    ("coverage", "üó∫Ô∏èüåà", "Workflow"),
    ("frequency", "üìäüîÅ", "Workflow"),
    ("unmapped", "‚ùìüö´", "Workflow"),
    ("mapped", "‚úÖüîó", "Workflow"),
    // --- Rust/Codebase Concepts ---
    ("crate", "üì¶ü¶Ä", "Rust"),
    ("module", "üì¶üß©", "Rust"),
    ("component", "üß©üèóÔ∏è", "Rust"),
    ("signal", "üì∂üîî", "Rust"),
    ("state", "üîîüß†", "Rust"),
    ("identifier", "üÜîüî§", "Rust"),
    ("literal", "üî§üíé", "Rust"),
    ("ast", "üå≥üß©", "Rust"),
    ("node", "üîóüå≥", "Rust"),
    ("token", "üîñüî§", "Rust"),
    ("snippet", "‚úÇÔ∏èüìÑ", "Rust"),
    ("template", "üìÑüß©", "Rust"),
    ("pattern", "üî∂üîç", "Rust"),
    ("function", "ü¶Ä‚öôÔ∏è", "Rust"),
    ("struct", "üèõÔ∏èüß±", "Rust"),
    ("trait", "üè∑Ô∏èüß¨", "Rust"),
    ("macro", "ü™Ñü¶Ä", "Rust"),
    ("prop", "üîëüè∑Ô∏è", "Rust"),
    ("emoji", "üòÄ‚ú®", "Rust"),
    ("vocabulary", "üìöüó£Ô∏è", "Rust"),
    // --- System/Architecture Concepts ---
    ("system", "üñß‚öôÔ∏è", "System"),
    ("framework", "üèóÔ∏èüåø", "System"),
    ("GUI", "üñ•Ô∏èüé®", "System"),
    ("MCP", "üß©üõ†Ô∏è", "System"),
    ("beauty", "üå∏‚ú®", "System"),
    ("layer", "üßÖüé®", "System"),
    ("expressive", "üé≠üí¨", "System"),
    ("rich", "üíéüåà", "System"),
    // --- Other/Playful Concepts ---
    ("brainrot", "üß†üå™Ô∏è", "Playful"),
    ("meme", "üòÇüìà", "Playful"),
    ("italian", "üáÆüáπüçù", "Playful"),
    ("prime", "üåüüî¢", "Playful"),
    ("idea", "üí°‚ú®", "Playful"),
    ("flow", "üåä‚û°Ô∏è", "Playful"),
    ("hero", "ü¶∏‚Äç‚ôÇÔ∏èüó∫Ô∏è", "Playful"),
    ("mythos", "üìúüåå", "Playful"),
];

fn emoji_for_type(ty: &str) -> (&'static str, &'static str) {
    for &(name, emoji, category) in EMOJI_TYPE_MAP {
        if ty == name {
            return (emoji, category);
        }
    }
    ("‚ùìü§∑", "Uncategorized")
}

fn extract_string_literals(value: &serde_json::Value, out: &mut Vec<String>) {
    match value {
        serde_json::Value::Object(map) => {
            for (k, v) in map.iter() {
                // Extract string literal values
                if (k == "lit" || k == "str") && v.is_string() {
                    if let Some(s) = v.as_str() {
                        out.push(s.to_string());
                    }
                }
                // Extract identifier-like fields
                if ["ident", "name", "label", "var", "field", "path", "ty", "type", "kind", "value"].contains(&k.as_str()) && v.is_string() {
                    if let Some(s) = v.as_str() {
                        out.push(s.to_string());
                    }
                }
                extract_string_literals(v, out);
            }
        },
        serde_json::Value::Array(arr) => {
            for v in arr {
                extract_string_literals(v, out);
            }
        },
        _ => {}
    }
}

fn split_words(s: &str) -> Vec<String> {
    // Split on whitespace, punctuation, underscores
    let mut words = Vec::new();
    let _re = Regex::new(r"[A-Za-z0-9]+_").unwrap(); // dummy, not used for splitting
    for part in s.split(|c: char| !c.is_alphanumeric() && c != '_') {
        if part.is_empty() { continue; }
        // Manually split CamelCase
        let mut last = 0;
        let chars: Vec<char> = part.chars().collect();
        for i in 1..chars.len() {
            if chars[i].is_uppercase() && chars[i - 1].is_lowercase() {
                words.push(chars[last..i].iter().collect::<String>().to_lowercase());
                last = i;
            }
        }
        if last < chars.len() {
            words.push(chars[last..].iter().collect::<String>().to_lowercase());
        }
    }
    words
}

fn main() {
    use std::env;
    let mut args = env::args().skip(1);
    let mut target_path: Option<String> = None;
    let mut limit: Option<usize> = None;
    while let Some(arg) = args.next() {
        if arg == "--limit" {
            if let Some(lim) = args.next() {
                limit = lim.parse().ok();
            }
        } else {
            target_path = Some(arg);
        }
    }

    let mut files = HashMap::new();
    let mut file_count = 0;
    let mut discovered_files = Vec::new();
    if let Some(ref path) = target_path {
        let path = Path::new(path);
        if path.is_file() {
            discovered_files.push(path.to_path_buf());
        } else if path.is_dir() {
            for entry in WalkDir::new(path).into_iter().filter_map(Result::ok) {
                if entry.file_type().is_file() {
                    discovered_files.push(entry.path().to_path_buf());
                }
            }
        } else {
            println!("[ERROR] Path not found: {}", path.display());
            return;
        }
    } else {
        for entry in WalkDir::new(".").into_iter().filter_map(Result::ok) {
            if entry.file_type().is_file() {
                discovered_files.push(entry.path().to_path_buf());
            }
        }
    }
    if let Some(lim) = limit {
        discovered_files.truncate(lim);
    }
    println!("[INFO] Processing {} files:", discovered_files.len());
    for f in &discovered_files {
        println!("  - {}", f.display());
    }
    for path in discovered_files {
        let ext = path.extension().and_then(|e| e.to_str()).unwrap_or("").to_lowercase();
        match ext.as_str() {
            // Text/code formats
            "rs" | "md" | "json" | "ttl" => {
                match fs::read_to_string(&path) {
                    Ok(content) => {
                        let tokens: Vec<&str> = content.split_whitespace().collect();
                        files.insert(path.to_string_lossy().to_string(), tokens.join(" "));
                        file_count += 1;
                    },
                    Err(e) => {
                        println!("[ERROR: could not read file: {}]", e);
                    }
                }
            },
            // Image formats
            "png" | "jpg" | "jpeg" | "gif" | "svg" => {
                println!("[TODO: Create AI description task for image file: {}]", path.display());
            },
            // Unknown/other formats
            _ => {
                match fs::read_to_string(&path) {
                    Ok(content) => {
                        let tokens: Vec<&str> = content.split_whitespace().collect();
                        files.insert(path.to_string_lossy().to_string(), tokens.join(" "));
                        file_count += 1;
                    },
                    Err(_) => {}
                }
            }
        }
        if let Some(lim) = limit {
            if file_count >= lim {
                break;
            }
        }
    }

    // 2. Create HF dataset structure early
    println!("\n[INFO] Creating Hugging Face dataset structure...");
    let dataset_dir = "hf_dataset";
    if !Path::new(dataset_dir).exists() {
        match fs::create_dir_all(dataset_dir) {
            Ok(_) => println!("[INFO] Created dataset directory: {}", dataset_dir),
            Err(e) => {
                println!("[ERROR] Could not create dataset directory: {}", e);
                return;
            }
        }
    }
    
    // Create HF reports directory
    let hf_reports_dir = format!("{}/reports", dataset_dir);
    if !Path::new(&hf_reports_dir).exists() {
        match fs::create_dir_all(&hf_reports_dir) {
            Ok(_) => println!("[INFO] Created HF reports directory: {}", hf_reports_dir),
            Err(e) => {
                println!("[ERROR] Could not create HF reports directory: {}", e);
                return;
            }
        }
    }

    // 3. Analyze all files
    println!("[INFO] Initializing CodeAnalyzer ...");
    let mut analyzer = CodeAnalyzer::new(32, 0.8);
    println!("[INFO] Analyzing files ...");
    let analyses = match analyzer.analyze_multiple_files(files) {
        Ok(a) => a,
        Err(e) => {
            println!("[ERROR] Failed to analyze files: {}", e);
            return;
        }
    };
    println!("[INFO] Analysis complete. {} files analyzed.", analyses.len());

    // 3. Set up reports directory
    let reports_dir = "reports";
    if !Path::new(reports_dir).exists() {
        match fs::create_dir_all(reports_dir) {
            Ok(_) => println!("[INFO] Created reports directory: {}", reports_dir),
            Err(e) => {
                println!("[ERROR] Could not create reports directory: {}", e);
                return;
            }
        }
    }

    fn count_types_recursive(value: &serde_json::Value, type_counts: &mut BTreeMap<String, usize>, total_nodes: &mut usize) {
        match value {
            serde_json::Value::Object(map) => {
                *total_nodes += 1;
                for (k, v) in map.iter() {
                    *type_counts.entry(k.clone()).or_insert(0) += 1;
                    count_types_recursive(v, type_counts, total_nodes);
                }
            },
            serde_json::Value::Array(arr) => {
                for v in arr {
                    count_types_recursive(v, type_counts, total_nodes);
                }
            },
            _ => {}
        }
    }
    let mut dir_type_counts: HashMap<String, BTreeMap<String, usize>> = HashMap::new();
    let mut total_type_counts: BTreeMap<String, usize> = BTreeMap::new();
    let mut global_word_counts: BTreeMap<String, usize> = BTreeMap::new();
    let mut global_word_emoji_counts: BTreeMap<String, usize> = BTreeMap::new();
    for (i, analysis) in analyses.iter().enumerate() {
        match serde_json::from_str::<serde_json::Value>(&analysis.json_ast) {
            Ok(ast) => {
                let mut type_counts = BTreeMap::new();
                let mut total_nodes = 0;
                count_types_recursive(&ast, &mut type_counts, &mut total_nodes);
                // Extract string literals and process words
                let mut string_literals = Vec::new();
                extract_string_literals(&ast, &mut string_literals);
                let mut word_counts = BTreeMap::new();
                for s in &string_literals {
                    for word in split_words(s) {
                        *word_counts.entry(word).or_insert(0) += 1;
                    }
                }
                // Map words (from literals and identifiers) to emojis
                let mut word_emoji_counts = BTreeMap::new();
                for (word, count) in &word_counts {
                    let (emoji, category) = emoji_for_type(word);
                    // Always record the emoji mapping, even for identifiers and module names
                    if emoji != "‚ùì" && emoji != "‚ùìü§∑" {
                        *word_emoji_counts.entry(emoji).or_insert(0usize) += *count;
                    }
                }
                // Count emojis in string literals
                let mut emoji_counts_in_strings = BTreeMap::new();
                for s in &string_literals {
                    for ch in s.chars() {
                        if ch.len_utf8() > 2 { // crude emoji filter
                            let e = ch.to_string();
                            *emoji_counts_in_strings.entry(e).or_insert(0) += 1;
                        }
                    }
                }
                // Write enriched report file directly to HF dataset
                let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
                let node_count = ast.as_object().map(|o| o.len()).unwrap_or(0);
                let report = serde_json::json!({
                    "file_path": analysis.file_path,
                    "timestamp": timestamp,
                    "summary": {
                        "top_level_nodes": node_count,
                        "total_nodes": total_nodes,
                        "type_counts": type_counts,
                        "string_literals": string_literals,
                        "word_counts": word_counts,
                        "word_emoji_counts": word_emoji_counts,
                        "emoji_counts_in_strings": emoji_counts_in_strings
                    },
                    "ast": ast
                });
                
                // Directory aggregation
                let dir = analysis.file_path.rsplit_once('/').map(|(d, _)| d).unwrap_or("");
                let dir_entry = dir_type_counts.entry(dir.to_string()).or_default();
                for (ty, count) in &type_counts {
                    *dir_entry.entry(ty.clone()).or_insert(0) += *count;
                    *total_type_counts.entry(ty.clone()).or_insert(0) += *count;
                }
                
                // Create compact directory structure for HF dataset reports
                let path_parts: Vec<&str> = analysis.file_path.split(['/', '\\']).collect();
                let subdir_name = if path_parts.len() >= 3 {
                    let name = format!("{}_{}_{}", path_parts[0], path_parts[1], path_parts[2]);
                    if name.len() > 50 { name[..50].to_string() } else { name }
                } else if path_parts.len() == 2 {
                    let name = format!("{}_{}", path_parts[0], path_parts[1]);
                    if name.len() > 50 { name[..50].to_string() } else { name }
                } else if path_parts.len() == 1 {
                    let name = path_parts[0].to_string();
                    if name.len() > 50 { name[..50].to_string() } else { name }
                } else {
                    "root".to_string()
                };
                
                // Create a shorter filename to avoid Windows path length limits
                let original_filename = path_parts.last().unwrap_or(&"unknown");
                let short_filename = if original_filename.len() > 30 {
                    // Truncate long filenames to 30 chars
                    format!("{}.json", &original_filename[..30])
                } else {
                    format!("{}.json", original_filename)
                };
                let hf_report_path = format!("{}/reports/{}/{}", dataset_dir, subdir_name, short_filename);
                
                // Create the subdirectory if it doesn't exist
                let subdir_path = format!("{}/reports/{}", dataset_dir, subdir_name);
                if !Path::new(&subdir_path).exists() {
                    if let Err(e) = fs::create_dir_all(&subdir_path) {
                        println!("[ERROR] Failed to create directory {}: {}", subdir_path, e);
                        continue;
                    }
                }
                
                let report_json = serde_json::to_string_pretty(&report).unwrap();
                
                // Write to HF dataset reports directory
                match fs::write(&hf_report_path, &report_json) {
                    Ok(_) => {
                        // Structure summary
                        let mut emoji_counts = Vec::new();
                        let mut emoji_summary = String::new();
                        for (ty, count) in &type_counts {
                            let (emoji, category) = emoji_for_type(ty);
                            emoji_counts.push(format!("{}({})√ó{}", emoji, ty, count));
                            emoji_summary.push_str(&emoji.repeat(*count.min(&10)));
                        }
                        let emoji_counts_str = emoji_counts.join(" ");
                        let filename = format!("{}.json", path_parts.last().unwrap_or(&"unknown"));
                        if type_counts.is_empty() {
                            println!("{} | none |", filename);
                        } else {
                            println!("{} | {} | {}", filename, emoji_counts_str, emoji_summary);
                        }
                        // Emojis found in string literals
                        if !emoji_counts_in_strings.is_empty() {
                            let mut emoji_strs = Vec::new();
                            for (emoji, count) in &emoji_counts_in_strings {
                                emoji_strs.push(format!("{}√ó{}", emoji, count));
                            }
                            println!("[emojis in strings] {}", emoji_strs.join(" "));
                        }
                        // Words mapped to emojis (from literals and identifiers)
                        if !word_emoji_counts.is_empty() {
                            let mut word_emoji_strs = Vec::new();
                            for (emoji, count) in &word_emoji_counts {
                                word_emoji_strs.push(format!("{}√ó{}", emoji, count));
                            }
                            println!("[words mapped to emojis] {}", word_emoji_strs.join(" "));
                        }
                        // Aggregate global word counts
                        for (word, count) in &word_counts {
                            *global_word_counts.entry(word.clone()).or_insert(0) += *count;
                        }
                        for (emoji, count) in &word_emoji_counts {
                            *global_word_emoji_counts.entry(emoji.to_string()).or_insert(0) += *count;
                        }
                    },
                    Err(e) => println!("[ERROR] Failed to write report {}: {}", hf_report_path, e),
                }
            },
            Err(e) => {
                println!("[ERROR] Failed to parse AST: {}", e);
            }
        }
    }
    // Print per-directory summary table
    println!("\n=== Directory Emoji Summary Table ===");
    let mut dir_keys: Vec<_> = dir_type_counts.keys().collect();
    dir_keys.sort();
    let mut global_dir_reports = Vec::new();
    for dir in dir_keys {
        let type_counts = &dir_type_counts[dir];
        let mut emoji_counts = Vec::new();
        let mut emoji_summary = String::new();
        for (ty, count) in type_counts {
            let (emoji, category) = emoji_for_type(ty);
            emoji_counts.push(format!("{}({})√ó{}", emoji, ty, count));
            emoji_summary.push_str(&emoji.repeat((*count).min(10)));
        }
        let emoji_counts_str = emoji_counts.join(" ");
        let mut report = String::new();
        report.push_str(&format!("=== Directory Emoji Summary: {} ===\n", dir));
        if type_counts.is_empty() {
            report.push_str(&format!("none\n"));
        } else {
            report.push_str(&format!("{} | {}\n", emoji_counts_str, emoji_summary));
        }
        // Per-directory word/category/emoji breakdown
        let mut dir_word_counts: BTreeMap<String, usize> = BTreeMap::new();
        let mut dir_word_emoji_counts: BTreeMap<String, usize> = BTreeMap::new();
        // Aggregate words for this directory
        for (i, analysis) in analyses.iter().enumerate() {
            if let Some(file_dir) = analysis.file_path.rsplit_once('/').map(|(d, _)| d) {
                if file_dir == dir {
                    if let Ok(ast) = serde_json::from_str::<serde_json::Value>(&analysis.json_ast) {
                        let mut string_literals = Vec::new();
                        extract_string_literals(&ast, &mut string_literals);
                        for s in &string_literals {
                            for word in split_words(s) {
                                *dir_word_counts.entry(word).or_insert(0) += 1;
                            }
                        }
                        for (word, count) in &dir_word_counts {
                            let (emoji, category) = emoji_for_type(word);
                            if emoji != "‚ùì" && emoji != "‚ùìü§∑" {
                                dir_word_emoji_counts.entry(emoji.to_string()).or_insert(0usize).saturating_add(*count);
                            }
                        }
                    }
                }
            }
        }
        // Word report
        report.push_str("\n=== Directory Word Report ===\n");
        report.push_str(&format!("{:<20} | {:<8} | {:<18} | {}\n", "word", "count", "category", "emoji"));
        let mut word_keys: Vec<_> = dir_word_counts.keys().collect();
        word_keys.sort();
        let mut found_agave = false;
        let mut found_css = false;
        let mut found_crypto = false;
        let mut found_version = false;
        for word in word_keys.iter() {
            let count = dir_word_counts[*word];
            let (emoji, category) = emoji_for_type(word);
            if *word == "agave" { found_agave = true; }
            if ["px", "deg", "em", "rem", "vh", "vw", "animation", "transition", "absolute", "align", "app", "app_state", "accessibility"].contains(&word.as_str()) { found_css = true; }
            if ["aead", "aeads", "aes", "argon2", "arc", "addr2line", "aarch64", "amd64", "armv8", "crypto", "curve25519", "ed25519", "elliptic", "fiat", "cbor"].contains(&word.as_str()) { found_crypto = true; }
            if ["zm", "h", "v"].contains(&word.as_str()) { found_version = true; }
            if emoji != "‚ùì" && emoji != "‚ùìü§∑" {
                report.push_str(&format!("{:<20} | {:<8} | {:<18} | {}\n", word, count, category, emoji));
            } else {
                report.push_str(&format!("{:<20} | {:<8} | {:<18} |\n", word, count, category));
            }
        }
        // Banners
        if found_agave {
            report.push_str("\nüåµüåµüåµ AGAVE detected! This project is spicy! üåµüåµüåµ\n");
        }
        if found_css {
            report.push_str("\nüé® CSS/Frontend detected! Styling and animation everywhere!\n");
        }
        if found_crypto {
            report.push_str("\nüîí Crypto detected! Security is strong in this codebase.\n");
        }
        if found_version {
            report.push_str("\nüî¢ Versioning/Hash detected! Lots of unique IDs and versions.\n");
        }
        // Write to file
        let safe_dir = if dir.is_empty() { "root".to_string() } else { dir.replace('/', "_") };
        let report_path = format!("{}/summary_{}.txt", reports_dir, safe_dir);
        match fs::write(&report_path, &report) {
            Ok(_) => println!("[INFO] Wrote directory summary to {}", report_path),
            Err(e) => println!("[ERROR] Failed to write directory summary {}: {}", report_path, e),
        }
        global_dir_reports.push((dir.clone(), report_path));
    }
    // Print total summary (minimal)
    let mut total_report = String::new();
    total_report.push_str("=== Total Emoji Summary Table ===\n");
    total_report.push_str(&format!("{:<20} | {:<8} | {:<18} | {}\n", "Type", "Count", "Category", "Emoji"));
    let mut type_keys: Vec<_> = total_type_counts.keys().collect();
    type_keys.sort();
    for ty in type_keys {
        let count = total_type_counts[ty];
        let (emoji, category) = emoji_for_type(ty);
        total_report.push_str(&format!("{:<20} | {:<8} | {:<18} | {}\n", ty, count, category, emoji));
    }
    total_report.push_str(&format!("\n[INFO] Total files processed: {}\n", analyses.len()));
    // Write total summary
    let merged_path = format!("{}/summary_total.txt", reports_dir);
    match fs::write(&merged_path, &total_report) {
        Ok(_) => println!("[INFO] Wrote total summary to {}", merged_path),
        Err(e) => println!("[ERROR] Failed to write total summary: {}", e),
    }

    // 5. Create Hugging Face Dataset Structure
    println!("\n[INFO] Creating Hugging Face dataset structure...");
    
    // Create dataset metadata
    let dataset_info = serde_json::json!({
        "description": "Rust codebase AST analysis with emoji mapping",
        "license": "agpl-3.0",
        "features": {
            "file_path": {"dtype": "string"},
            "timestamp": {"dtype": "int64"},
            "ast": {"dtype": "string"},
            "summary": {
                "dtype": "map",
                "mapping": {
                    "top_level_nodes": {"dtype": "int64"},
                    "total_nodes": {"dtype": "int64"},
                    "type_counts": {"dtype": "map"},
                    "string_literals": {"dtype": "sequence", "feature": {"dtype": "string"}},
                    "word_counts": {"dtype": "map"},
                    "word_emoji_counts": {"dtype": "map"},
                    "emoji_counts_in_strings": {"dtype": "map"}
                }
            }
        },
        "builder_name": "rust_ast_emoji",
        "config_name": "default",
        "version": {"version_str": "0.1.0"},
        "splits": {
            "train": {
                "name": "train",
                "num_bytes": 0,
                "num_examples": 0,
                "shard_lengths": []
            }
        }
    });

    // Write dataset info
    let info_path = format!("{}/dataset_info.json", dataset_dir);
    match fs::write(&info_path, serde_json::to_string_pretty(&dataset_info).unwrap()) {
        Ok(_) => println!("[INFO] Wrote dataset info to {}", info_path),
        Err(e) => println!("[ERROR] Failed to write dataset info: {}", e),
    }

    // Split ASTs into chunks and organize into subdirectories
    let max_file_size = 1024 * 1024; // 1MB
    let max_files_per_dir = 10000;
    let mut current_chunk = Vec::new();
    let mut current_chunk_size = 0;
    let mut chunk_index = 0;
    let mut file_index = 0;
    let mut total_examples = 0;

    // Create data directory
    let data_dir = format!("{}/data", dataset_dir);
    if !Path::new(&data_dir).exists() {
        match fs::create_dir_all(&data_dir) {
            Ok(_) => println!("[INFO] Created data directory: {}", data_dir),
            Err(e) => {
                println!("[ERROR] Could not create data directory: {}", e);
                return;
            }
        }
    }

    // Copy reports to dataset
    let reports_data_dir = format!("{}/reports", dataset_dir);
    if !Path::new(&reports_data_dir).exists() {
        match fs::create_dir_all(&reports_data_dir) {
            Ok(_) => println!("[INFO] Created reports directory: {}", reports_data_dir),
            Err(e) => {
                println!("[ERROR] Could not create reports directory: {}", e);
                return;
            }
        }
    }


    
    // Write summary files directly to HF dataset
    let summary_files = [
        "summary_total.txt",
        "emoji_mapping.txt"
    ];
    
    for summary_file in &summary_files {
        let source_path = format!("{}/{}", reports_dir, summary_file);
        let target_path = format!("{}/reports/{}", dataset_dir, summary_file);
        
        if Path::new(&source_path).exists() {
            match fs::copy(&source_path, &target_path) {
                Ok(_) => println!("[INFO] Copied summary file: {}", summary_file),
                Err(e) => println!("[ERROR] Failed to copy {}: {}", summary_file, e),
            }
        }
    }

    // Process each analysis and create chunks
    for analysis in &analyses {
        if let Ok(ast) = serde_json::from_str::<serde_json::Value>(&analysis.json_ast) {
            let mut type_counts = BTreeMap::new();
            let mut total_nodes = 0;
            count_types_recursive(&ast, &mut type_counts, &mut total_nodes);
            
            let mut string_literals = Vec::new();
            extract_string_literals(&ast, &mut string_literals);
            let mut word_counts = BTreeMap::new();
            for s in &string_literals {
                for word in split_words(s) {
                    *word_counts.entry(word).or_insert(0) += 1;
                }
            }
            
            let mut word_emoji_counts = BTreeMap::new();
            for (word, count) in &word_counts {
                let (emoji, category) = emoji_for_type(word);
                if emoji != "‚ùì" && emoji != "‚ùìü§∑" {
                    *word_emoji_counts.entry(emoji).or_insert(0usize) += *count;
                }
            }
            
            let mut emoji_counts_in_strings = BTreeMap::new();
            for s in &string_literals {
                for ch in s.chars() {
                    if ch.len_utf8() > 2 {
                        let e = ch.to_string();
                        *emoji_counts_in_strings.entry(e).or_insert(0) += 1;
                    }
                }
            }

            let example = serde_json::json!({
                "file_path": analysis.file_path,
                "timestamp": SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs(),
                "ast": ast,
                "summary": {
                    "top_level_nodes": ast.as_object().map(|o| o.len()).unwrap_or(0),
                    "total_nodes": total_nodes,
                    "type_counts": type_counts,
                    "string_literals": string_literals,
                    "word_counts": word_counts,
                    "word_emoji_counts": word_emoji_counts,
                    "emoji_counts_in_strings": emoji_counts_in_strings
                }
            });

            // Calculate the actual size this example will add to the chunk
            let example_json = serde_json::to_string(&example).unwrap();
            let example_size = example_json.len();
            
            // Debug: Print size information
            if example_size > 1024 * 1024 { // If any single example is > 1MB
                println!("[WARNING] Large example: {} bytes for {}", example_size, analysis.file_path);
            }

            // Check if adding this example would exceed the chunk size
            if current_chunk_size + example_size > max_file_size && !current_chunk.is_empty() {
                // Write current chunk
                let subdir = file_index / max_files_per_dir;
                let subdir_path = format!("{}/{:03}", data_dir, subdir);
                if !Path::new(&subdir_path).exists() {
                    match fs::create_dir_all(&subdir_path) {
                        Ok(_) => println!("[INFO] Created subdirectory: {}", subdir_path),
                        Err(e) => println!("[ERROR] Failed to create subdirectory: {}", e),
                    }
                }

                let chunk_path = format!("{}/chunk_{:05}.json", subdir_path, chunk_index);
                let chunk_data = serde_json::json!({
                    "examples": current_chunk,
                    "metadata": {
                        "chunk_index": chunk_index,
                        "num_examples": current_chunk.len(),
                        "total_size_bytes": current_chunk_size
                    }
                });

                match fs::write(&chunk_path, serde_json::to_string(&chunk_data).unwrap()) {
                    Ok(_) => println!("[INFO] Wrote chunk {} to {} ({} examples, {} bytes)", chunk_index, chunk_path, current_chunk.len(), current_chunk_size),
                    Err(e) => println!("[ERROR] Failed to write chunk {}: {}", chunk_index, e),
                }

                // Reset for next chunk
                current_chunk.clear();
                current_chunk_size = 0;
                chunk_index += 1;
            }

            // Add example to current chunk
            current_chunk.push(example);
            current_chunk_size += example_size;
            file_index += 1;
            total_examples += 1;
        }
    }

    // Write final chunk if not empty
    if !current_chunk.is_empty() {
        let subdir = file_index / max_files_per_dir;
        let subdir_path = format!("{}/{:03}", data_dir, subdir);
        if !Path::new(&subdir_path).exists() {
            match fs::create_dir_all(&subdir_path) {
                Ok(_) => println!("[INFO] Created subdirectory: {}", subdir_path),
                Err(e) => println!("[ERROR] Failed to create subdirectory: {}", e),
            }
        }

        let chunk_path = format!("{}/chunk_{:05}.json", subdir_path, chunk_index);
        let chunk_data = serde_json::json!({
            "examples": current_chunk,
            "metadata": {
                "chunk_index": chunk_index,
                "num_examples": current_chunk.len(),
                "total_size_bytes": current_chunk_size
            }
        });

        match fs::write(&chunk_path, serde_json::to_string(&chunk_data).unwrap()) {
            Ok(_) => println!("[INFO] Wrote final chunk {} to {}", chunk_index, chunk_path),
            Err(e) => println!("[ERROR] Failed to write final chunk {}: {}", chunk_index, e),
        }
    }

    // Create README for the dataset
    let readme_content = format!("# Rust AST Emoji Dataset

This dataset contains Rust codebase AST (Abstract Syntax Tree) analysis with emoji mapping for code understanding and visualization.

## Dataset Structure

- **Total Examples**: {}
- **Total Chunks**: {}
- **Max File Size**: 10KB per chunk
- **Max Files per Directory**: 10,000

## Features

- `file_path`: Path to the original Rust source file
- `timestamp`: Unix timestamp of analysis
- `ast`: Full AST representation in JSON
- `summary`: Analysis summary including:
  - `top_level_nodes`: Number of top-level AST nodes
  - `total_nodes`: Total number of AST nodes
  - `type_counts`: Count of each AST node type
  - `string_literals`: Extracted string literals
  - `word_counts`: Word frequency analysis
  - `word_emoji_counts`: Emoji mapping for words
  - `emoji_counts_in_strings`: Emojis found in string literals

## Usage

This dataset can be used for:
- Code understanding and visualization
- AST pattern analysis
- Emoji-based code summarization
- Codebase domain detection (Crypto, Web, i18n, etc.)

## License

AGPL-3.0 License
", total_examples, chunk_index + 1);

    let readme_path = format!("{}/README.md", dataset_dir);
    match fs::write(&readme_path, readme_content) {
        Ok(_) => println!("[INFO] Wrote README to {}", readme_path),
        Err(e) => println!("[ERROR] Failed to write README: {}", e),
    }

    println!("[INFO] Hugging Face dataset created successfully in '{}'", dataset_dir);
    println!("[INFO] Dataset contains {} examples across {} chunks", total_examples, chunk_index + 1);
}

// Function to write emoji ontology as Turtle (RDF)
fn write_emoji_ontology_turtle(path: &str) {
    use std::fs::File;
    use std::io::Write;
    let mut file = File::create(path).expect("Failed to create emoji ontology turtle file");
    writeln!(file, "@prefix em: <http://example.org/emoji#> .").ok();
    for (term, emoji, category) in EMOJI_TYPE_MAP.iter() {
        writeln!(file, "em:{} a em:Concept ; em:emoji \"{}\" ; em:category \"{}\" .", term.replace('-', "_"), emoji, category).ok();
    }
}

// Function to import emoji ontology from Turtle (RDF) file
fn import_emoji_ontology_turtle(path: &str) -> std::collections::HashMap<String, (String, String)> {
    use std::fs::File;
    use std::io::{BufRead, BufReader};
    use std::collections::HashMap;
    let file = File::open(path).expect("Failed to open emoji ontology turtle file");
    let reader = BufReader::new(file);
    let mut map = HashMap::new();
    let mut current_term = None;
    let mut emoji = None;
    let mut category = None;
    for line in reader.lines() {
        let line = line.unwrap();
        if line.starts_with("em:") {
            // Parse subject
            if let Some(idx) = line.find(' ') {
                let term = line[3..idx].replace('_', "-");
                current_term = Some(term);
            }
        }
        if let Some(start) = line.find("em:emoji \"") {
            let rest = &line[start + 10..];
            if let Some(end) = rest.find('"') {
                emoji = Some(rest[..end].to_string());
            }
        }
        if let Some(start) = line.find("em:category \"") {
            let rest = &line[start + 13..];
            if let Some(end) = rest.find('"') {
                category = Some(rest[..end].to_string());
            }
        }
        if line.trim_end().ends_with('.') {
            if let (Some(term), Some(e), Some(cat)) = (current_term.take(), emoji.take(), category.take()) {
                map.insert(term, (e, cat));
            }
        }
    }
    map
}

// Example: Using Sophia to read and write the emoji ontology Turtle file
#[allow(dead_code)]
fn sophia_read_write_emoji_ontology() -> Result<(), Box<dyn std::error::Error>> {
    // Use the vendored Sophia path
    use sophia_turtle::parser::turtle;
    use sophia_api::graph::Graph;
    use sophia_inmem::graph::FastGraph;
    use sophia_api::ns::Namespace;
    //use sophia_api::term::SimpleIri;
    
    use sophia_api::prelude::TripleSource;
    use sophia_api::prelude::TripleSerializer;
    use std::fs::File;
    use std::io::{BufReader, BufWriter};
    use sophia_api::prelude::Triple;
    use sophia_api::graph::MutableGraph;
    use sophia_api::ns::NsTerm;
    
    // Read the ontology
    let file = File::open("reports/emoji_ontology.ttl")?;
    let reader = BufReader::new(file);
    let mut graph: FastGraph = turtle::parse_bufread(reader).collect_triples()?;

    // Example: Iterate over all emoji concepts
    let em = Namespace::new("http://example.org/emoji#")?;
    let emoji_pred = em.get("emoji")?;
    for triple in graph.triples_matching(None::<&NsTerm>, Some(&emoji_pred), None::<&NsTerm>) {
        let t = triple?;
        println!("Term: {:?}, Emoji: {:?}", t.s(), t.o());
    }

    // Example: Add a new triple
    let new_term = em.get("example_term")?;
    let emoji_pred = em.get("emoji")?;
    let emoji_val = "ü¶Ä";
    graph.insert(&new_term, &emoji_pred, emoji_val)?;

    // Write the updated ontology
    let out_file = File::create("reports/emoji_ontology_out.ttl")?;
    let mut writer = BufWriter::new(out_file);
    //sophia_turtle::serializer::turtle::TurtleConfig::new()
    //.serialize_graph(&graph, &mut writer)?;
    let mut serializer = sophia_turtle::serializer::turtle::TurtleSerializer::new_with_config(&mut writer, sophia_turtle::serializer::turtle::TurtleConfig::new());  
    serializer.serialize_triples(graph.triples())?;

    
    Ok(())
}
